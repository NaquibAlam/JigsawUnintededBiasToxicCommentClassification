{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/pretrained-bert-including-scripts/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import required packages\n#basics\nimport pandas as pd \nimport numpy as np\n\n#misc\nimport gc\nimport time\nimport warnings\n\n#stats\nfrom scipy.misc import imread\nfrom scipy import sparse\nimport scipy.stats as ss\n\n#viz\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec \nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom PIL import Image\n\n#nlp\nimport string\nimport re    #for regex\nimport nltk, os\nfrom nltk.corpus import stopwords\n# import spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \nfrom keras.preprocessing.text import Tokenizer\n\n\n\n#FeatureEngineering\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\n\n\nimport keras\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport os, re, json\nfrom gensim.models import KeyedVectors\nfrom wordcloud import WordCloud, STOPWORDS\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom operator import itemgetter\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D, GRU, concatenate\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, Dropout, SpatialDropout1D, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.callbacks import LearningRateScheduler\n\n\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\nfrom nltk.stem.lancaster import LancasterStemmer\nlc = LancasterStemmer()\nfrom nltk.stem import SnowballStemmer\nsb = SnowballStemmer(\"english\")\nimport gc\nimport sys","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"!pip install keras_bert","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install bert-tensorflow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras_bert.bert import get_model\nfrom keras_bert.loader import load_trained_model_from_checkpoint\nfrom keras.optimizers import Adam\nadam = Adam(lr=2e-5,decay=0.01)\nmaxlen = 36\nprint('begin_build')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sys.path.insert(0, '../input/pretrained-bert-including-scripts/master/bert-master')\n!cp -r '../input/keras_bert' \n# !cp -r '{path}data/bert\nBERT_PRETRAINED_DIR = '../input/pretrained-bert-including-scripts/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\nprint('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\nfrom bert import tokenization  #Actually keras_bert contains tokenization part, here just for conv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = 2e-5\nweight_decay = 0.001\nnb_epochs=1\nbsz = 32\nmaxlen=72\n## Training data\ntrain_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntrain_df = train_df.sample(frac=0.1,random_state = 42)\n#train_df['comment_text'] = train_df['comment_text'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True)\ntrain_lines, train_labels = train_df['comment_text'].values, train_df.target.values \n## step parameter \ndecay_steps = int(nb_epochs*train_lines.shape[0]/bsz)\nwarmup_steps = int(0.1*decay_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\ncheckpoint_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\nmodel = load_trained_model_from_checkpoint(config_file, checkpoint_file, training=True, seq_len=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary(line_length=120)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras_bert import AdamWarmup, calc_train_steps\n\ntotal_steps, warmup_steps = calc_train_steps(\n    num_example=train_df.shape[0],\n    batch_size=32,\n    epochs=1,\n    warmup_proportion=0.1,\n)\n\noptimizer = AdamWarmup(total_steps, warmup_steps, lr=2e-5, min_lr=0.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense,Input,Flatten,concatenate,Dropout,Lambda\nfrom keras.models import Model\nimport keras.backend as K\nimport re\nimport codecs\n# adamwarm = AdamWarmup(lr=lr,decay_steps = decay_steps, warmup_steps = warmup_steps,kernel_weight_decay = weight_decay)\nsequence_output  = model.layers[-6].output\npool_output = Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),name = 'real_output')(sequence_output)\nmodel3  = Model(inputs=model.input, outputs=pool_output)\nmodel3.compile(loss='binary_crossentropy', optimizer=optimizer)\nmodel3.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = [weight.name for layer in model3.layers for weight in layer.weights]\nweights = model3.get_weights()\n\nfor name, weight in zip(names, weights):\n    print(name, weight.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for i in range(example.shape[0]):\n      tokens_a = tokenizer.tokenize(example[i])\n      if len(tokens_a)>max_seq_length:\n        tokens_a = tokens_a[:max_seq_length]\n        longer += 1\n      one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n      all_tokens.append(one_token)\n    print(longer)\n    return np.array(all_tokens)\n    \n\ndict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\ntokenizer = tokenization.FullTokenizer(vocab_file=dict_path, do_lower_case=True)\nprint('build tokenizer done')\nprint('sample used',train_lines.shape)\ntoken_input = convert_lines(train_lines,maxlen,tokenizer)\nseg_input = np.zeros((token_input.shape[0],maxlen))\nmask_input = np.ones((token_input.shape[0],maxlen))\nprint(token_input.shape)\nprint(seg_input.shape)\nprint(mask_input.shape)\nprint('begin training')\nmodel3.fit([token_input, seg_input, mask_input],train_labels,batch_size=bsz,epochs=nb_epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load test data\ntest_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n#test_df['comment_text'] = test_df['comment_text'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True)\neval_lines = test_df['comment_text'].values\nprint(eval_lines.shape)\nprint('load data done')\ntoken_input2 = convert_lines(eval_lines,maxlen,tokenizer)\nseg_input2 = np.zeros((token_input2.shape[0],maxlen))\nmask_input2 = np.ones((token_input2.shape[0],maxlen))\nprint('test data done')\nprint(token_input2.shape)\nprint(seg_input2.shape)\nprint(mask_input2.shape)\nhehe = model3.predict([token_input2, seg_input2, mask_input2],verbose=1,batch_size=bsz)\nsubmission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')\nsubmission['prediction'] = hehe\nsubmission.reset_index(drop=False, inplace=True)\nsubmission.to_csv('submission_adamwarmup.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}